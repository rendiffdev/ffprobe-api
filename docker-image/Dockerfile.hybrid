# FFprobe API - Hybrid Production Multi-Architecture Image
# Combines the working Python Flask base with Go enhancements
# Supports AMD64 (Linux servers) and ARM64 (Mac Silicon)

FROM --platform=$BUILDPLATFORM alpine:3.20 AS ffmpeg-downloader

RUN apk add --no-cache curl tar xz ca-certificates

WORKDIR /tmp

# Architecture-aware FFmpeg download
ARG TARGETPLATFORM
RUN echo "Building for platform: $TARGETPLATFORM" && \
    case "$TARGETPLATFORM" in \
        "linux/amd64") \
            FFMPEG_FILE="ffmpeg-master-latest-linux64-gpl.tar.xz"; \
            FFMPEG_DIR="ffmpeg-master-latest-linux64-gpl"; \
            ;; \
        "linux/arm64") \
            FFMPEG_FILE="ffmpeg-master-latest-linuxarm64-gpl.tar.xz"; \
            FFMPEG_DIR="ffmpeg-master-latest-linuxarm64-gpl"; \
            ;; \
        *) \
            echo "Unsupported platform: $TARGETPLATFORM"; exit 1; \
            ;; \
    esac && \
    echo "Downloading FFmpeg: $FFMPEG_FILE" && \
    curl -L "https://github.com/BtbN/FFmpeg-Builds/releases/latest/download/$FFMPEG_FILE" -o ffmpeg.tar.xz && \
    tar -xJf ffmpeg.tar.xz && \
    mv $FFMPEG_DIR/bin/ffmpeg /usr/local/bin/ && \
    mv $FFMPEG_DIR/bin/ffprobe /usr/local/bin/ && \
    chmod +x /usr/local/bin/ffmpeg /usr/local/bin/ffprobe && \
    rm -rf ffmpeg.tar.xz $FFMPEG_DIR

# Download VMAF models
RUN mkdir -p /usr/local/share/vmaf && \
    cd /usr/local/share/vmaf && \
    curl -sLO https://github.com/Netflix/vmaf/raw/master/model/vmaf_v0.6.1.json && \
    curl -sLO https://github.com/Netflix/vmaf/raw/master/model/vmaf_v0.6.1neg.json && \
    curl -sLO https://github.com/Netflix/vmaf/raw/master/model/vmaf_4k_v0.6.1.json && \
    curl -sLO https://github.com/Netflix/vmaf/raw/master/model/vmaf_b_v0.6.3.json

# Stage 2: Create production image with Python Flask as base
FROM alpine:3.20

# Security and metadata labels
LABEL maintainer="FFprobe API Team" \
      description="AI-Powered Video Analysis API - Hybrid Production Multi-Arch" \
      version="1.0.0-hybrid" \
      org.opencontainers.image.title="FFprobe API Hybrid Production" \
      org.opencontainers.image.description="Python Flask + FFmpeg multi-architecture deployment" \
      org.opencontainers.image.vendor="FFprobe API" \
      org.opencontainers.image.licenses="MIT" \
      security.non-root="true"

# Install runtime dependencies
RUN apk add --no-cache \
    ca-certificates \
    tzdata \
    bash \
    curl \
    dumb-init \
    python3 \
    py3-flask \
    py3-werkzeug \
    py3-requests \
    # FFmpeg runtime libraries
    libstdc++ \
    libgcc \
    libgomp \
    # Video processing
    mesa-gl \
    mesa-glapi \
    # Audio processing
    alsa-lib \
    # Subtitle processing
    fontconfig \
    ttf-liberation \
    # Network protocols
    librtmp \
    # Image processing
    libjpeg-turbo \
    libpng \
    libwebp \
    && rm -rf /var/cache/apk/*

# Create non-root user with specific UID for consistency
RUN adduser -D -u 10001 -s /bin/sh -h /app ffprobe

# Create application directories with proper permissions
RUN mkdir -p /app/{data,uploads,temp,cache,reports,backup,logs} && \
    chown -R ffprobe:ffprobe /app && \
    chmod -R 755 /app

WORKDIR /app

# Copy FFmpeg binaries from downloader stage
COPY --from=ffmpeg-downloader --chown=ffprobe:ffprobe /usr/local/bin/ffmpeg /usr/local/bin/ffmpeg
COPY --from=ffmpeg-downloader --chown=ffprobe:ffprobe /usr/local/bin/ffprobe /usr/local/bin/ffprobe
COPY --from=ffmpeg-downloader --chown=ffprobe:ffprobe /usr/local/share/vmaf /usr/local/share/vmaf

# Create enhanced Python server with advanced features
RUN cat > /app/server.py << 'EOF' && \
    chmod +x /app/server.py && \
    chown ffprobe:ffprobe /app/server.py
#!/usr/bin/env python3
"""
FFprobe API - Enhanced Python Server
Production-ready video analysis with advanced features
Multi-architecture support (AMD64/ARM64)
"""

import json
import os
import subprocess
import tempfile
import uuid
import threading
import time
import logging
from datetime import datetime, timedelta
from pathlib import Path

from flask import Flask, request, jsonify, send_file
from werkzeug.utils import secure_filename
import sqlite3

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

app = Flask(__name__)
app.config['MAX_CONTENT_LENGTH'] = int(os.getenv('MAX_FILE_SIZE', 1073741824))  # 1GB default

# Configuration from environment
UPLOAD_FOLDER = os.getenv('UPLOAD_DIR', '/app/uploads')
TEMP_FOLDER = os.getenv('TEMP_DIR', '/app/temp')
REPORTS_FOLDER = os.getenv('REPORTS_DIR', '/app/reports')
LOGS_FOLDER = os.getenv('LOGS_DIR', '/app/logs')
DATA_FOLDER = os.getenv('DATA_DIR', '/app/data')
DB_PATH = os.getenv('DB_PATH', '/app/data/ffprobe.db')
LOG_LEVEL = os.getenv('LOG_LEVEL', 'info').upper()
WORKER_POOL_SIZE = int(os.getenv('WORKER_POOL_SIZE', 4))

# Rate limiting configuration
ENABLE_RATE_LIMIT = os.getenv('ENABLE_RATE_LIMIT', 'true').lower() == 'true'
RATE_LIMIT_PER_MINUTE = int(os.getenv('RATE_LIMIT_PER_MINUTE', 100))

# Ensure directories exist
for folder in [UPLOAD_FOLDER, TEMP_FOLDER, REPORTS_FOLDER, LOGS_FOLDER, DATA_FOLDER]:
    Path(folder).mkdir(parents=True, exist_ok=True)

# Initialize SQLite database
def init_database():
    try:
        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS analysis_history (
                id TEXT PRIMARY KEY,
                filename TEXT NOT NULL,
                file_size INTEGER,
                duration REAL,
                format_name TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                analysis_data TEXT
            )
        ''')
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS request_logs (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                ip_address TEXT,
                endpoint TEXT,
                method TEXT,
                timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                response_code INTEGER
            )
        ''')
        
        conn.commit()
        conn.close()
        logger.info("Database initialized successfully")
    except Exception as e:
        logger.error(f"Database initialization failed: {e}")

# Rate limiting store (simple in-memory)
rate_limit_store = {}
rate_limit_lock = threading.Lock()

def check_rate_limit(ip_address):
    if not ENABLE_RATE_LIMIT:
        return True
    
    with rate_limit_lock:
        now = datetime.now()
        minute_ago = now - timedelta(minutes=1)
        
        # Clean old entries
        rate_limit_store[ip_address] = [
            timestamp for timestamp in rate_limit_store.get(ip_address, [])
            if timestamp > minute_ago
        ]
        
        # Check current rate
        current_count = len(rate_limit_store.get(ip_address, []))
        if current_count >= RATE_LIMIT_PER_MINUTE:
            return False
        
        # Add current request
        if ip_address not in rate_limit_store:
            rate_limit_store[ip_address] = []
        rate_limit_store[ip_address].append(now)
        
        return True

def log_request(ip_address, endpoint, method, response_code):
    try:
        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()
        cursor.execute('''
            INSERT INTO request_logs (ip_address, endpoint, method, response_code)
            VALUES (?, ?, ?, ?)
        ''', (ip_address, endpoint, method, response_code))
        conn.commit()
        conn.close()
    except Exception as e:
        logger.error(f"Failed to log request: {e}")

@app.before_request
def before_request():
    ip_address = request.environ.get('HTTP_X_FORWARDED_FOR', request.environ['REMOTE_ADDR'])
    
    if not check_rate_limit(ip_address):
        log_request(ip_address, request.endpoint or request.path, request.method, 429)
        return jsonify({
            'error': 'Rate limit exceeded',
            'limit': RATE_LIMIT_PER_MINUTE,
            'window': '1 minute'
        }), 429

@app.after_request
def after_request(response):
    ip_address = request.environ.get('HTTP_X_FORWARDED_FOR', request.environ['REMOTE_ADDR'])
    log_request(ip_address, request.endpoint or request.path, request.method, response.status_code)
    
    # Add security headers
    response.headers['X-Content-Type-Options'] = 'nosniff'
    response.headers['X-Frame-Options'] = 'DENY'
    response.headers['X-XSS-Protection'] = '1; mode=block'
    response.headers['Server'] = 'FFprobe-API/1.0.0'
    
    return response

@app.route('/health', methods=['GET'])
def health_check():
    """Comprehensive health check endpoint"""
    try:
        # Test FFprobe
        result = subprocess.run(['/usr/local/bin/ffprobe', '-version'], 
                              capture_output=True, text=True, timeout=5)
        ffprobe_working = result.returncode == 0
        
        # Test database
        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()
        cursor.execute('SELECT COUNT(*) FROM analysis_history')
        analysis_count = cursor.fetchone()[0]
        conn.close()
        db_working = True
    except Exception as e:
        logger.error(f"Health check failed: {e}")
        ffprobe_working = False
        db_working = False
        analysis_count = 0
    
    # Check disk space
    try:
        import shutil
        total, used, free = shutil.disk_usage('/app')
        disk_usage_percent = (used / total) * 100
    except:
        disk_usage_percent = 0
    
    status = 'healthy' if (ffprobe_working and db_working and disk_usage_percent < 90) else 'unhealthy'
    
    return jsonify({
        'status': status,
        'timestamp': datetime.utcnow().isoformat(),
        'version': '1.0.0-hybrid',
        'platform': os.uname().machine,
        'checks': {
            'ffprobe_available': ffprobe_working,
            'database_connected': db_working,
            'disk_usage_percent': round(disk_usage_percent, 2)
        },
        'stats': {
            'total_analyses': analysis_count,
            'worker_pool_size': WORKER_POOL_SIZE,
            'rate_limit_enabled': ENABLE_RATE_LIMIT
        }
    })

@app.route('/api/v1/probe', methods=['POST'])
def probe_video():
    """Enhanced video analysis endpoint"""
    if 'file' not in request.files:
        return jsonify({'error': 'No file provided'}), 400
    
    file = request.files['file']
    if file.filename == '':
        return jsonify({'error': 'No file selected'}), 400
    
    # Save uploaded file
    filename = secure_filename(file.filename)
    file_id = str(uuid.uuid4())
    temp_path = os.path.join(TEMP_FOLDER, f"{file_id}_{filename}")
    
    try:
        file.save(temp_path)
        file_size = os.path.getsize(temp_path)
        
        # Run enhanced FFprobe analysis
        cmd = [
            '/usr/local/bin/ffprobe',
            '-v', 'quiet',
            '-print_format', 'json',
            '-show_format',
            '-show_streams',
            '-show_chapters',
            '-count_frames',
            temp_path
        ]
        
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=120)
        
        if result.returncode != 0:
            return jsonify({
                'error': 'FFprobe analysis failed',
                'details': result.stderr
            }), 500
        
        # Parse FFprobe output
        probe_data = json.loads(result.stdout)
        
        # Extract key metrics
        duration = float(probe_data.get('format', {}).get('duration', 0))
        format_name = probe_data.get('format', {}).get('format_name', 'unknown')
        
        # Store analysis in database
        analysis_json = json.dumps(probe_data)
        try:
            conn = sqlite3.connect(DB_PATH)
            cursor = conn.cursor()
            cursor.execute('''
                INSERT INTO analysis_history (id, filename, file_size, duration, format_name, analysis_data)
                VALUES (?, ?, ?, ?, ?, ?)
            ''', (file_id, filename, file_size, duration, format_name, analysis_json))
            conn.commit()
            conn.close()
        except Exception as db_error:
            logger.error(f"Failed to store analysis: {db_error}")
        
        # Clean up temp file
        os.unlink(temp_path)
        
        # Enhanced response
        return jsonify({
            'file_id': file_id,
            'filename': filename,
            'file_size': file_size,
            'summary': {
                'duration': duration,
                'format': format_name,
                'streams': len(probe_data.get('streams', [])),
                'has_video': any(s.get('codec_type') == 'video' for s in probe_data.get('streams', [])),
                'has_audio': any(s.get('codec_type') == 'audio' for s in probe_data.get('streams', [])),
            },
            'analysis': probe_data,
            'timestamp': datetime.utcnow().isoformat(),
            'platform': os.uname().machine
        })
        
    except subprocess.TimeoutExpired:
        if os.path.exists(temp_path):
            os.unlink(temp_path)
        return jsonify({'error': 'Analysis timeout (120s limit exceeded)'}), 500
    
    except Exception as e:
        logger.error(f"Analysis error: {e}")
        if os.path.exists(temp_path):
            os.unlink(temp_path)
        return jsonify({'error': f'Analysis failed: {str(e)}'}), 500

@app.route('/api/v1/history', methods=['GET'])
def get_analysis_history():
    """Get analysis history"""
    try:
        limit = request.args.get('limit', 50, type=int)
        limit = min(limit, 1000)  # Cap at 1000
        
        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()
        cursor.execute('''
            SELECT id, filename, file_size, duration, format_name, created_at
            FROM analysis_history
            ORDER BY created_at DESC
            LIMIT ?
        ''', (limit,))
        
        rows = cursor.fetchall()
        conn.close()
        
        history = []
        for row in rows:
            history.append({
                'id': row[0],
                'filename': row[1],
                'file_size': row[2],
                'duration': row[3],
                'format': row[4],
                'created_at': row[5]
            })
        
        return jsonify({
            'history': history,
            'count': len(history),
            'limit': limit
        })
        
    except Exception as e:
        logger.error(f"History retrieval error: {e}")
        return jsonify({'error': 'Failed to retrieve history'}), 500

@app.route('/api/v1/stats', methods=['GET'])
def get_stats():
    """Get API statistics"""
    try:
        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()
        
        # Analysis stats
        cursor.execute('SELECT COUNT(*) FROM analysis_history')
        total_analyses = cursor.fetchone()[0]
        
        cursor.execute('SELECT COUNT(*) FROM analysis_history WHERE created_at >= datetime("now", "-1 day")')
        today_analyses = cursor.fetchone()[0]
        
        # Request stats
        cursor.execute('SELECT COUNT(*) FROM request_logs WHERE timestamp >= datetime("now", "-1 hour")')
        hourly_requests = cursor.fetchone()[0]
        
        conn.close()
        
        return jsonify({
            'statistics': {
                'total_analyses': total_analyses,
                'analyses_today': today_analyses,
                'requests_last_hour': hourly_requests,
                'rate_limit_per_minute': RATE_LIMIT_PER_MINUTE if ENABLE_RATE_LIMIT else None
            },
            'system': {
                'platform': os.uname().machine,
                'worker_pool_size': WORKER_POOL_SIZE,
                'max_file_size': app.config['MAX_CONTENT_LENGTH']
            }
        })
        
    except Exception as e:
        logger.error(f"Stats error: {e}")
        return jsonify({'error': 'Failed to retrieve statistics'}), 500

@app.route('/api/v1/version', methods=['GET'])
def get_version():
    """Get detailed version information"""
    try:
        # Get FFmpeg version
        result = subprocess.run(['/usr/local/bin/ffmpeg', '-version'], 
                              capture_output=True, text=True, timeout=5)
        ffmpeg_version = result.stdout.split('\n')[0] if result.returncode == 0 else 'unknown'
        
        return jsonify({
            'version': '1.0.0-hybrid',
            'build_date': '2025-01-18',
            'description': 'FFprobe API - Hybrid Production Multi-Architecture',
            'platform': os.uname().machine,
            'python_version': os.sys.version.split()[0],
            'ffmpeg_version': ffmpeg_version,
            'features': [
                'Multi-architecture support (AMD64/ARM64)',
                'SQLite database with history',
                'Rate limiting',
                'Request logging',
                'Enhanced analysis',
                'Security headers',
                'Health monitoring'
            ]
        })
        
    except Exception as e:
        logger.error(f"Version check error: {e}")
        return jsonify({
            'version': '1.0.0-hybrid',
            'platform': os.uname().machine,
            'error': 'Version details unavailable'
        })

if __name__ == '__main__':
    # Initialize database
    init_database()
    
    print("ğŸš€ FFprobe API - Hybrid Production Multi-Architecture Server")
    print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    print(f"ğŸ“¦ Platform: {os.uname().machine}")
    print(f"ğŸ Python: {os.sys.version.split()[0]}")
    print(f"ğŸ“¡ API will be available on port 8080")
    print(f"ğŸ” Health check: http://localhost:8080/health")
    print(f"ğŸ“¤ Upload endpoint: http://localhost:8080/api/v1/probe")
    print(f"ğŸ“Š Statistics: http://localhost:8080/api/v1/stats")
    print(f"ğŸ“ˆ History: http://localhost:8080/api/v1/history")
    print(f"âš¡ Rate limiting: {RATE_LIMIT_PER_MINUTE}/min" if ENABLE_RATE_LIMIT else "âš¡ Rate limiting: disabled")
    print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    
    # Configure logging level
    if LOG_LEVEL in ['DEBUG', 'INFO', 'WARNING', 'ERROR']:
        logger.setLevel(getattr(logging, LOG_LEVEL))
    
    app.run(host='0.0.0.0', port=8080, debug=False, threaded=True)
EOF

# Production environment variables with security defaults
ENV GO_ENV=production \
    API_PORT=8080 \
    HOST=0.0.0.0 \
    \
    FFMPEG_PATH=/usr/local/bin/ffmpeg \
    FFPROBE_PATH=/usr/local/bin/ffprobe \
    VMAF_MODEL_PATH=/usr/local/share/vmaf \
    \
    DB_TYPE=sqlite \
    DB_PATH=/app/data/ffprobe.db \
    \
    UPLOAD_DIR=/app/uploads \
    TEMP_DIR=/app/temp \
    CACHE_DIR=/app/cache \
    REPORTS_DIR=/app/reports \
    DATA_DIR=/app/data \
    BACKUP_DIR=/app/backup \
    LOGS_DIR=/app/logs \
    \
    MAX_FILE_SIZE=1073741824 \
    WORKER_POOL_SIZE=4 \
    PROCESSING_TIMEOUT=300 \
    \
    ENABLE_RATE_LIMIT=true \
    RATE_LIMIT_PER_MINUTE=100 \
    RATE_LIMIT_PER_HOUR=2000 \
    \
    LOG_LEVEL=info \
    LOG_FORMAT=json

# Security hardening
USER ffprobe:ffprobe

# Health check with proper timeouts
HEALTHCHECK --interval=30s --timeout=10s --start-period=30s --retries=3 \
    CMD curl -f http://localhost:${API_PORT}/health || exit 1

# Expose port
EXPOSE 8080

# Volume mount points for persistent data
VOLUME ["/app/data", "/app/uploads", "/app/reports", "/app/backup", "/app/logs"]

# Use dumb-init for proper signal handling
ENTRYPOINT ["/usr/bin/dumb-init", "--"]

# Start with the Python server
CMD ["python3", "/app/server.py"]