# Zero-Configuration FFprobe API Deployment
# Ready to use immediately - just run: docker compose up -d
# No manual configuration required - works out of the box!

name: ffprobe-api

services:
  # FFprobe API Service - Fully Preconfigured
  ffprobe-api:
    image: rendiffdev/ffprobe-api:latest
    container_name: ffprobe-api
    restart: unless-stopped
    ports:
      - "8080:8080"
    environment:
      # Database Configuration (SQLite embedded - zero config required)
      - DB_TYPE=sqlite
      - DB_PATH=/app/data/ffprobe.db
      
      # Cache Configuration (Valkey - Redis compatible)
      - VALKEY_HOST=valkey
      - VALKEY_PORT=6379
      - VALKEY_PASSWORD=ffprobe-cache-2024
      
      # Performance Configuration (optimized for immediate use)
      - WORKER_POOL_SIZE=8
      - PROCESSING_TIMEOUT=600
      - MAX_FILE_SIZE=21474836480
      
      # Security Configuration (disabled for easy setup)
      - ENABLE_AUTH=false
      - ENABLE_RATE_LIMIT=true
      - RATE_LIMIT_PER_MINUTE=100
      - RATE_LIMIT_PER_HOUR=5000
      
      # AI Configuration (enabled with auto-downloaded models)
      - ENABLE_LOCAL_LLM=true
      - OLLAMA_URL=http://ollama:11434
      - OLLAMA_MODEL=gemma3:270m
      - OLLAMA_FALLBACK_MODEL=phi3:mini
      
      # Application Configuration
      - GO_ENV=production
      - GIN_MODE=release
      - LOG_LEVEL=info
    volumes:
      - ffprobe_uploads:/app/uploads
      - ffprobe_reports:/app/reports
      - ffprobe_data:/app/data
      - ffprobe_backup:/app/backup
    depends_on:
      valkey:
        condition: service_healthy
      ollama:
        condition: service_started
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 60s
    networks:
      - ffprobe-network

  # Valkey Cache (Redis-compatible, open source)
  valkey:
    image: valkey/valkey:8.0-alpine
    container_name: ffprobe-valkey
    restart: unless-stopped
    command: >
      valkey-server
      --requirepass ffprobe-cache-2024
      --appendonly yes
      --save 900 1 --save 300 10 --save 60 10000
      --maxmemory 512mb
      --maxmemory-policy allkeys-lru
    volumes:
      - valkey_data:/data
    healthcheck:
      test: ["CMD", "valkey-cli", "-a", "ffprobe-cache-2024", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    networks:
      - ffprobe-network

  # Ollama AI Service (Auto-downloads models on first run)
  ollama:
    image: ollama/ollama:latest
    container_name: ffprobe-ollama
    restart: unless-stopped
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0:11434
      - OLLAMA_KEEP_ALIVE=10m
    deploy:
      resources:
        limits:
          memory: 4G
        reservations:
          memory: 2G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/version"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s
    entrypoint: ["/bin/sh", "-c"]
    command: |
      "echo 'Starting Ollama AI service...' && 
       ollama serve & 
       echo 'Waiting for Ollama to be ready...' && 
       sleep 15 && 
       echo 'Auto-downloading AI models for immediate use...' && 
       ollama pull gemma3:270m && 
       ollama pull phi3:mini && 
       echo 'âœ… AI models ready! FFprobe API is fully operational.' && 
       tail -f /dev/null"
    networks:
      - ffprobe-network

networks:
  ffprobe-network:
    driver: bridge
    labels:
      - "com.ffprobe-api.network=main"

volumes:
  # Named volumes for data persistence
  ffprobe_data:
    driver: local
    labels:
      - "com.ffprobe-api.volume=database"
  ffprobe_uploads:
    driver: local
    labels:
      - "com.ffprobe-api.volume=uploads"
  ffprobe_reports:
    driver: local
    labels:
      - "com.ffprobe-api.volume=reports"
  ffprobe_backup:
    driver: local
    labels:
      - "com.ffprobe-api.volume=backup"
  valkey_data:
    driver: local
    labels:
      - "com.ffprobe-api.volume=cache"
  ollama_data:
    driver: local
    labels:
      - "com.ffprobe-api.volume=ai-models"