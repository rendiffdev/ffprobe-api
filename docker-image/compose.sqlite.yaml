# FFprobe API - SQLite-only Deployment (Zero External Dependencies)
# Minimal Docker Compose with embedded SQLite database

name: ffprobe-api-sqlite

services:
  # Main API service with SQLite database
  api:
    build:
      context: .
      dockerfile: docker-image/Dockerfile
      target: production
    image: ffprobe-api:sqlite
    container_name: ffprobe-api-sqlite
    hostname: ffprobe-api
    ports:
      - "8080:8080"
    environment:
      GO_ENV: ${GO_ENV:-production}
      API_PORT: 8080
      HOST: 0.0.0.0
      
      # SQLite Database Configuration
      DB_TYPE: sqlite
      DB_PATH: /app/data/ffprobe.db
      
      # Optional Valkey (can be disabled for file-based cache)
      VALKEY_HOST: ${VALKEY_HOST:-valkey}
      VALKEY_PORT: 6379
      VALKEY_PASSWORD: ${VALKEY_PASSWORD:-}
      
      # AI/LLM Configuration
      ENABLE_LOCAL_LLM: ${ENABLE_LOCAL_LLM:-true}
      OLLAMA_URL: http://ollama:11434
      OLLAMA_MODEL: ${OLLAMA_MODEL:-gemma3:270m}
      OLLAMA_FALLBACK_MODEL: ${OLLAMA_FALLBACK_MODEL:-phi3:mini}
      
      # Performance and limits
      WORKER_POOL_SIZE: ${WORKER_POOL_SIZE:-8}
      MAX_FILE_SIZE: ${MAX_FILE_SIZE:-10737418240}
      PROCESSING_TIMEOUT: ${PROCESSING_TIMEOUT:-600}
      
      # Security (optional)
      ENABLE_AUTH: ${ENABLE_AUTH:-false}
      API_KEY: ${API_KEY:-}
      ENABLE_RATE_LIMIT: ${ENABLE_RATE_LIMIT:-true}
      
      # Storage
      UPLOAD_DIR: /app/uploads
      REPORTS_DIR: /app/reports
      TEMP_DIR: /app/temp
    env_file:
      - path: .env
        required: false
    volumes:
      - ./data/sqlite:/app/data
      - ./uploads:/app/uploads
      - ./reports:/app/reports
      - ./temp:/app/temp
      - ./cache:/app/cache
    depends_on:
      valkey:
        condition: service_healthy
        restart: true
      ollama:
        condition: service_started
        restart: true
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        limits:
          cpus: '4'
          memory: 4G
        reservations:
          cpus: '1'
          memory: 1G
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"
    networks:
      - ffprobe-network

  # Valkey for caching (Redis-compatible, open source alternative)
  valkey:
    image: valkey/valkey:8.0-alpine
    container_name: ffprobe-valkey-sqlite
    hostname: valkey
    command: >
      valkey-server
      --requirepass ${VALKEY_PASSWORD:-}
      --appendonly yes
      --maxmemory 256mb
      --maxmemory-policy allkeys-lru
      --save 900 1
      --save 300 10
      --save 60 10000
    volumes:
      - ./data/valkey:/data
    ports:
      - "${VALKEY_PORT:-6379}:6379"
    profiles: ["cache", "full"]
    healthcheck:
      test: ["CMD", "valkey-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 3
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          cpus: '0.1'
          memory: 64M
    networks:
      - ffprobe-network

  # Ollama AI service (optional for GenAI features)
  ollama:
    image: ollama/ollama:latest
    container_name: ffprobe-ollama-sqlite
    hostname: ollama
    environment:
      OLLAMA_HOST: 0.0.0.0:11434
      OLLAMA_MODELS: ${OLLAMA_MODEL:-gemma3:270m}
      OLLAMA_NUM_PARALLEL: 2
      OLLAMA_KEEP_ALIVE: 5m
    volumes:
      - ./data/ollama:/root/.ollama
    ports:
      - "11434:11434"
    profiles: ["ai", "full"]
    deploy:
      resources:
        limits:
          cpus: '4'
          memory: 4G
        reservations:
          cpus: '1'
          memory: 1G
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/version"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 180s
    entrypoint: ["/bin/sh"]
    command: >
      -c "
        echo 'Starting Ollama service...'
        ollama serve &
        OLLAMA_PID=$$!
        
        echo 'Waiting for Ollama to be ready...'
        timeout=60
        while [ $$timeout -gt 0 ]; do
          if curl -s http://localhost:11434/api/version >/dev/null 2>&1; then
            echo 'Ollama is ready!'
            break
          fi
          sleep 2
          timeout=$$((timeout-1))
        done
        
        if [ $$timeout -gt 0 ]; then
          echo 'Downloading AI model: ${OLLAMA_MODEL:-gemma3:270m}'
          ollama pull ${OLLAMA_MODEL:-gemma3:270m} || echo 'Model download failed (will work offline if model exists)'
          
          echo 'Available models:'
          ollama list
        fi
        
        echo 'Ollama ready! Keeping service running...'
        wait $$OLLAMA_PID
      "
    networks:
      - ffprobe-network

networks:
  ffprobe-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.21.0.0/16

# Deployment profiles for different use cases:
# docker compose -f compose.sqlite.yaml up                    # API only with SQLite
# docker compose -f compose.sqlite.yaml --profile cache up   # API + Redis cache
# docker compose -f compose.sqlite.yaml --profile ai up      # API + AI features
# docker compose -f compose.sqlite.yaml --profile full up    # API + Cache + AI