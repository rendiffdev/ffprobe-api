# FFprobe API - Production-Optimized Docker Compose
# Enterprise-grade container orchestration with enhanced security, monitoring, and performance
# Version: 3.9 (Docker Compose File Format)

version: '3.9'
name: rendiff-probe-prod

# =============================================================================
# Production Services with Enhanced Configuration
# =============================================================================
services:
  # Main API Service - Load Balanced with Auto-Scaling
  api:
    build:
      context: ..
      dockerfile: docker-image/Dockerfile.optimized
      target: production
      args:
        VERSION: ${VERSION:-latest}
        COMMIT: ${COMMIT:-unknown}
        BUILD_DATE: ${BUILD_DATE}
        BUILDKIT_INLINE_CACHE: 1
      cache_from:
        - rendiff-probe:latest
        - rendiff-probe:cache
    image: rendiff-probe:${VERSION:-latest}
    hostname: api-{{.Task.Slot}}
    networks:
      - frontend
      - backend
      - monitoring
    ports:
      - target: 8080
        mode: host
    environment:
      # Core application settings
      GO_ENV: production
      GIN_MODE: release
      API_PORT: 8080
      API_HOST: 0.0.0.0
      LOG_LEVEL: ${LOG_LEVEL:-info}
      LOG_FORMAT: json
      
      # Database configuration
      DB_TYPE: sqlite
      DB_PATH: /app/data/rendiff-probe.db
      DB_MAX_CONNECTIONS: 25
      DB_CONNECTION_LIFETIME: 1h
      
      # Cache configuration (Valkey)
      VALKEY_HOST: valkey
      VALKEY_PORT: 6379
      VALKEY_PASSWORD_FILE: /run/secrets/valkey_password
      VALKEY_DB: 0
      VALKEY_POOL_SIZE: 20
      VALKEY_TIMEOUT: 5s
      
      # AI/LLM configuration
      OLLAMA_HOST: ollama:11434
      OLLAMA_TIMEOUT: 60s
      OLLAMA_MODEL: ${OLLAMA_MODEL:-gemma3:270m}
      OLLAMA_FALLBACK_MODEL: ${OLLAMA_FALLBACK_MODEL:-phi3:mini}
      
      # Security settings
      ENABLE_AUTH: true
      ENABLE_RATE_LIMIT: true
      ENABLE_CSRF: true
      ENABLE_CORS: true
      JWT_SECRET_FILE: /run/secrets/jwt_secret
      CSRF_SECRET_FILE: /run/secrets/csrf_secret
      
      # Performance optimization
      WORKER_POOL_SIZE: ${WORKER_POOL_SIZE:-16}
      MAX_CONCURRENT_JOBS: ${MAX_CONCURRENT_JOBS:-8}
      REQUEST_TIMEOUT: 30s
      UPLOAD_MAX_SIZE: 500MB
      
      # Monitoring and observability
      METRICS_ENABLED: true
      METRICS_PORT: 9090
      TRACING_ENABLED: true
      JAEGER_ENDPOINT: http://jaeger:14268/api/traces
      OTEL_SERVICE_NAME: rendiff-probe
      OTEL_SERVICE_VERSION: ${VERSION:-latest}
      
      # Feature flags
      ENABLE_CIRCUIT_BREAKER: true
      ENABLE_GRACEFUL_SHUTDOWN: true
      SHUTDOWN_TIMEOUT: 30s
      
    secrets:
      - valkey_password
      - jwt_secret
      - csrf_secret
      - api_key
    volumes:
      - type: volume
        source: app_data
        target: /app/data
        volume:
          nocopy: true
      - type: volume
        source: app_uploads
        target: /app/uploads
      - type: volume
        source: app_reports
        target: /app/reports
      - type: volume
        source: app_cache
        target: /app/cache
      - type: tmpfs
        target: /app/temp
        tmpfs:
          size: 1G
          mode: 0755
    deploy:
      mode: replicated
      replicas: ${API_REPLICAS:-3}
      placement:
        max_replicas_per_node: 2
        preferences:
          - spread: node.labels.zone
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
          pids: 1000
        reservations:
          cpus: '0.5'
          memory: 512M
      restart_policy:
        condition: any
        delay: 10s
        max_attempts: 5
        window: 120s
      update_config:
        parallelism: 1
        delay: 30s
        failure_action: rollback
        monitor: 60s
        order: start-first
        max_failure_ratio: 0.3
      rollback_config:
        parallelism: 1
        delay: 10s
        failure_action: pause
        monitor: 30s
        order: stop-first
    healthcheck:
      test: ["/app/healthcheck.sh"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
      start_interval: 5s
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "5"
        labels: "service,version,environment,instance"
        tag: "{{.ImageName}}|{{.Name}}|{{.ImageFullID}}|{{.FullID}}"
    security_opt:
      - no-new-privileges:true
      - seccomp:unconfined
    cap_drop:
      - ALL
    cap_add:
      - CHOWN
      - DAC_OVERRIDE
      - FOWNER
      - SETGID
      - SETUID
    read_only: true
    tmpfs:
      - /tmp:rw,noexec,nosuid,size=100M
    labels:
      # Traefik configuration
      - "traefik.enable=true"
      - "traefik.docker.network=frontend"
      - "traefik.http.routers.api-prod.rule=Host(`${API_DOMAIN:-api.localhost}`)"
      - "traefik.http.routers.api-prod.entrypoints=websecure"
      - "traefik.http.routers.api-prod.tls=true"
      - "traefik.http.routers.api-prod.tls.certresolver=letsencrypt"
      - "traefik.http.services.api-prod.loadbalancer.server.port=8080"
      - "traefik.http.services.api-prod.loadbalancer.healthcheck.path=/health"
      - "traefik.http.services.api-prod.loadbalancer.healthcheck.interval=30s"
      # Rate limiting
      - "traefik.http.middlewares.api-ratelimit.ratelimit.average=100"
      - "traefik.http.middlewares.api-ratelimit.ratelimit.burst=50"
      - "traefik.http.routers.api-prod.middlewares=api-ratelimit,secure-headers"
      # Security headers
      - "traefik.http.middlewares.secure-headers.headers.sslredirect=true"
      - "traefik.http.middlewares.secure-headers.headers.stsSeconds=31536000"
      - "traefik.http.middlewares.secure-headers.headers.stsIncludeSubdomains=true"
      - "traefik.http.middlewares.secure-headers.headers.stsPreload=true"
      - "traefik.http.middlewares.secure-headers.headers.forceSTSHeader=true"
      - "traefik.http.middlewares.secure-headers.headers.frameDeny=true"
      - "traefik.http.middlewares.secure-headers.headers.contentTypeNosniff=true"
      - "traefik.http.middlewares.secure-headers.headers.browserXssFilter=true"
      - "traefik.http.middlewares.secure-headers.headers.referrerPolicy=strict-origin-when-cross-origin"
      # Service metadata
      - "com.rendiff-probe.service=api"
      - "com.rendiff-probe.version=${VERSION:-latest}"
      - "com.rendiff-probe.environment=production"

  # High-Performance Valkey Cache Cluster
  valkey:
    image: valkey/valkey:8.0-alpine
    hostname: valkey
    networks:
      - backend
    command: >
      valkey-server
      --requirepass-file /run/secrets/valkey_password
      --appendonly yes
      --appendfsync everysec
      --save 900 1 --save 300 10 --save 60 10000
      --maxmemory ${VALKEY_MAX_MEMORY:-2gb}
      --maxmemory-policy allkeys-lru
      --tcp-keepalive 300
      --timeout 0
      --tcp-backlog 511
      --databases 16
      --maxclients 10000
      --lazyfree-lazy-eviction yes
      --lazyfree-lazy-expire yes
      --lazyfree-lazy-server-del yes
      --replica-lazy-flush yes
    secrets:
      - valkey_password
    volumes:
      - type: volume
        source: valkey_data
        target: /data
      - type: tmpfs
        target: /tmp
        tmpfs:
          size: 100M
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.labels.cache == true
      resources:
        limits:
          cpus: '1.5'
          memory: 3G
        reservations:
          cpus: '0.5'
          memory: 1G
      restart_policy:
        condition: any
        delay: 5s
        max_attempts: 3
    healthcheck:
      test: ["CMD", "valkey-cli", "--askpass", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    logging:
      driver: json-file
      options:
        max-size: "5m"
        max-file: "3"
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    read_only: true
    tmpfs:
      - /tmp:rw,noexec,nosuid,size=100M
    labels:
      - "com.rendiff-probe.service=cache"
      - "com.rendiff-probe.component=valkey"
      - "com.rendiff-probe.environment=production"

  # Optimized Ollama AI Service with GPU Support
  ollama:
    image: ollama/ollama:latest
    hostname: ollama
    networks:
      - backend
    environment:
      OLLAMA_HOST: 0.0.0.0:11434
      OLLAMA_MODELS: ${OLLAMA_MODEL:-gemma3:270m},${OLLAMA_FALLBACK_MODEL:-phi3:mini}
      OLLAMA_NUM_PARALLEL: ${OLLAMA_PARALLEL:-4}
      OLLAMA_MAX_QUEUE: 2048
      OLLAMA_KEEP_ALIVE: ${OLLAMA_KEEP_ALIVE:-15m}
      OLLAMA_FLASH_ATTENTION: true
      OLLAMA_MAX_LOADED_MODELS: 3
    volumes:
      - type: volume
        source: ollama_models
        target: /root/.ollama
      - type: tmpfs
        target: /tmp
        tmpfs:
          size: 8G
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.labels.gpu == true
      resources:
        limits:
          cpus: '8.0'
          memory: 16G
        reservations:
          cpus: '2.0'
          memory: 4G
        devices:
          - driver: nvidia
            count: 1
            capabilities: [gpu]
      restart_policy:
        condition: any
        delay: 30s
        max_attempts: 3
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/version"]
      interval: 60s
      timeout: 30s
      retries: 3
      start_period: 300s
    logging:
      driver: json-file
      options:
        max-size: "20m"
        max-file: "3"
    security_opt:
      - no-new-privileges:true
    labels:
      - "com.rendiff-probe.service=ai"
      - "com.rendiff-probe.component=ollama"
      - "com.rendiff-probe.environment=production"

  # Production Traefik with Enhanced Security
  traefik:
    image: traefik:v3.0
    hostname: traefik
    networks:
      - frontend
      - monitoring
    ports:
      - target: 80
        published: 80
        protocol: tcp
        mode: host
      - target: 443
        published: 443
        protocol: tcp
        mode: host
    command:
      # API and Dashboard
      - "--api.dashboard=true"
      - "--api.debug=false"
      - "--log.level=${TRAEFIK_LOG_LEVEL:-INFO}"
      - "--accesslog=true"
      - "--accesslog.format=json"
      
      # Docker provider
      - "--providers.docker=true"
      - "--providers.docker.endpoint=unix:///var/run/docker.sock"
      - "--providers.docker.exposedbydefault=false"
      - "--providers.docker.network=frontend"
      - "--providers.docker.swarmMode=true"
      
      # Entry points
      - "--entrypoints.web.address=:80"
      - "--entrypoints.websecure.address=:443"
      
      # SSL/TLS with Let's Encrypt
      - "--certificatesresolvers.letsencrypt.acme.tlschallenge=true"
      - "--certificatesresolvers.letsencrypt.acme.email=${ACME_EMAIL}"
      - "--certificatesresolvers.letsencrypt.acme.storage=/letsencrypt/acme.json"
      - "--certificatesresolvers.letsencrypt.acme.caserver=${ACME_CA_SERVER:-https://acme-v02.api.letsencrypt.org/directory}"
      
      # HTTP to HTTPS redirect
      - "--entrypoints.web.http.redirections.entrypoint.to=websecure"
      - "--entrypoints.web.http.redirections.entrypoint.scheme=https"
      
      # Security
      - "--entrypoints.websecure.http.tls.options=modern@file"
      - "--serversTransport.insecureSkipVerify=false"
      
      # Metrics
      - "--metrics.prometheus=true"
      - "--metrics.prometheus.addEntryPointsLabels=true"
      - "--metrics.prometheus.addServicesLabels=true"
      - "--metrics.prometheus.addRoutersLabels=true"
      
      # Tracing
      - "--tracing.jaeger=true"
      - "--tracing.jaeger.localAgentHostPort=jaeger:6831"
      
    volumes:
      - type: bind
        source: /var/run/docker.sock
        target: /var/run/docker.sock
        read_only: true
      - type: volume
        source: traefik_acme
        target: /letsencrypt
      - type: bind
        source: ./config/traefik
        target: /etc/traefik
        read_only: true
    deploy:
      mode: global
      placement:
        constraints:
          - node.role == manager
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.2'
          memory: 256M
      restart_policy:
        condition: any
        delay: 5s
        max_attempts: 3
    healthcheck:
      test: ["CMD", "traefik", "healthcheck", "--ping"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"
    security_opt:
      - no-new-privileges:true
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.dashboard.rule=Host(`traefik.${DOMAIN:-localhost}`)"
      - "traefik.http.routers.dashboard.tls=true"
      - "traefik.http.routers.dashboard.service=api@internal"
      - "traefik.http.routers.dashboard.middlewares=auth"
      - "traefik.http.middlewares.auth.basicauth.users=${TRAEFIK_AUTH}"

  # Comprehensive Monitoring Stack
  prometheus:
    image: prom/prometheus:latest
    hostname: prometheus
    networks:
      - monitoring
      - backend
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=30d'
      - '--storage.tsdb.retention.size=50GB'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--web.enable-lifecycle'
      - '--web.enable-admin-api'
      - '--web.external-url=https://prometheus.${DOMAIN:-localhost}'
      - '--log.level=${PROMETHEUS_LOG_LEVEL:-info}'
    volumes:
      - type: bind
        source: ./config/prometheus
        target: /etc/prometheus
        read_only: true
      - type: volume
        source: prometheus_data
        target: /prometheus
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.labels.monitoring == true
      resources:
        limits:
          cpus: '2.0'
          memory: 4G
        reservations:
          cpus: '0.5'
          memory: 1G
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9090/-/ready"]
      interval: 30s
      timeout: 10s
      retries: 3
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.prometheus.rule=Host(`prometheus.${DOMAIN:-localhost}`)"
      - "traefik.http.routers.prometheus.tls=true"

  grafana:
    image: grafana/grafana:latest
    hostname: grafana
    networks:
      - monitoring
      - backend
    environment:
      GF_SECURITY_ADMIN_PASSWORD__FILE: /run/secrets/grafana_password
      GF_SERVER_DOMAIN: grafana.${DOMAIN:-localhost}
      GF_SERVER_ROOT_URL: https://grafana.${DOMAIN:-localhost}
      GF_INSTALL_PLUGINS: redis-datasource,grafana-clock-panel,grafana-worldmap-panel,grafana-piechart-panel
      GF_FEATURE_TOGGLES_ENABLE: publicDashboards
      GF_DATABASE_TYPE: sqlite3
      GF_DATABASE_PATH: /var/lib/grafana/grafana.db
      GF_USERS_ALLOW_SIGN_UP: false
      GF_SECURITY_ALLOW_EMBEDDING: true
      GF_AUTH_ANONYMOUS_ENABLED: false
    secrets:
      - grafana_password
    volumes:
      - type: volume
        source: grafana_data
        target: /var/lib/grafana
      - type: bind
        source: ./config/grafana
        target: /etc/grafana/provisioning
        read_only: true
    deploy:
      mode: replicated
      replicas: 1
      resources:
        limits:
          cpus: '1.0'
          memory: 2G
        reservations:
          cpus: '0.2'
          memory: 512M
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.grafana.rule=Host(`grafana.${DOMAIN:-localhost}`)"
      - "traefik.http.routers.grafana.tls=true"

  # Distributed Tracing
  jaeger:
    image: jaegertracing/all-in-one:latest
    hostname: jaeger
    networks:
      - monitoring
      - backend
    environment:
      COLLECTOR_ZIPKIN_HOST_PORT: 9411
      COLLECTOR_OTLP_ENABLED: true
    deploy:
      mode: replicated
      replicas: 1
      resources:
        limits:
          cpus: '0.5'
          memory: 1G
        reservations:
          cpus: '0.1'
          memory: 256M
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.jaeger.rule=Host(`jaeger.${DOMAIN:-localhost}`)"
      - "traefik.http.routers.jaeger.tls=true"
      - "traefik.http.services.jaeger.loadbalancer.server.port=16686"

  # Automated Backup Service with Encryption
  backup:
    image: alpine:latest
    hostname: backup
    networks:
      - backend
    environment:
      BACKUP_SCHEDULE: ${BACKUP_SCHEDULE:-0 2 * * *}
      BACKUP_RETENTION_DAYS: ${BACKUP_RETENTION_DAYS:-30}
      BACKUP_ENCRYPTION_KEY_FILE: /run/secrets/backup_encryption_key
      S3_BUCKET: ${BACKUP_S3_BUCKET}
      S3_ENDPOINT: ${BACKUP_S3_ENDPOINT}
      AWS_ACCESS_KEY_ID_FILE: /run/secrets/s3_access_key
      AWS_SECRET_ACCESS_KEY_FILE: /run/secrets/s3_secret_key
    secrets:
      - backup_encryption_key
      - s3_access_key
      - s3_secret_key
    volumes:
      - type: volume
        source: app_data
        target: /backup/data
        read_only: true
      - type: volume
        source: valkey_data
        target: /backup/valkey
        read_only: true
      - type: bind
        source: ./scripts/backup
        target: /scripts
        read_only: true
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.labels.backup == true
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.1'
          memory: 128M
    command: >
      sh -c "
        apk add --no-cache aws-cli sqlite gnupg dcron &&
        echo '$BACKUP_SCHEDULE /scripts/backup.sh' | crontab - &&
        crond -f
      "

# =============================================================================
# Production Networks with Security Isolation
# =============================================================================
networks:
  frontend:
    driver: overlay
    external: true
    attachable: false
    scope: swarm
    labels:
      - "com.rendiff-probe.network.type=frontend"
      - "com.rendiff-probe.network.security=high"

  backend:
    driver: overlay
    internal: true
    attachable: false
    scope: swarm
    driver_opts:
      encrypted: "true"
    labels:
      - "com.rendiff-probe.network.type=backend"
      - "com.rendiff-probe.network.security=critical"

  monitoring:
    driver: overlay
    attachable: false
    scope: swarm
    labels:
      - "com.rendiff-probe.network.type=monitoring"
      - "com.rendiff-probe.network.security=medium"

# =============================================================================
# Production Volumes with Backup Labels
# =============================================================================
volumes:
  app_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${DATA_PATH}/app
    labels:
      - "com.rendiff-probe.backup=daily"
      - "com.rendiff-probe.volume.type=application-data"

  app_uploads:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${DATA_PATH}/uploads
    labels:
      - "com.rendiff-probe.backup=daily"
      - "com.rendiff-probe.volume.type=user-uploads"

  app_reports:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${DATA_PATH}/reports
    labels:
      - "com.rendiff-probe.backup=weekly"
      - "com.rendiff-probe.volume.type=generated-reports"

  app_cache:
    driver: local
    labels:
      - "com.rendiff-probe.backup=none"
      - "com.rendiff-probe.volume.type=cache"

  valkey_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${DATA_PATH}/valkey
    labels:
      - "com.rendiff-probe.backup=daily"
      - "com.rendiff-probe.volume.type=cache-data"

  ollama_models:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${DATA_PATH}/ollama
    labels:
      - "com.rendiff-probe.backup=weekly"
      - "com.rendiff-probe.volume.type=ai-models"

  traefik_acme:
    driver: local
    labels:
      - "com.rendiff-probe.backup=daily"
      - "com.rendiff-probe.volume.type=certificates"

  prometheus_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${DATA_PATH}/prometheus
    labels:
      - "com.rendiff-probe.backup=weekly"
      - "com.rendiff-probe.volume.type=metrics"

  grafana_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${DATA_PATH}/grafana
    labels:
      - "com.rendiff-probe.backup=weekly"
      - "com.rendiff-probe.volume.type=dashboards"

# =============================================================================
# Production Secrets Management
# =============================================================================
secrets:
  valkey_password:
    external: true
    name: valkey_password_v1

  jwt_secret:
    external: true
    name: jwt_secret_v1

  csrf_secret:
    external: true
    name: csrf_secret_v1

  api_key:
    external: true
    name: api_key_v1

  grafana_password:
    external: true
    name: grafana_password_v1

  backup_encryption_key:
    external: true
    name: backup_encryption_key_v1

  s3_access_key:
    external: true
    name: s3_access_key_v1

  s3_secret_key:
    external: true
    name: s3_secret_key_v1

# =============================================================================
# Production Configuration Templates
# =============================================================================
configs:
  traefik_config:
    external: true
    name: traefik_config_v1

  prometheus_config:
    external: true
    name: prometheus_config_v1

  grafana_dashboards:
    external: true
    name: grafana_dashboards_v1